{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88cf33c-567b-4722-9438-a7c5e7a0b1f6",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "1. Concept of attention was introduced in [Bahdanau paper](https://arxiv.org/abs/1409.0473). Idea was to improve Recurrent Neural networks used in the context of machine translation, so that we do not translate word-by-word, but rather have access to all sequence elements at each time step.\n",
    "   ![sentence translation](img/sentence.png)\n",
    "2. Moreover, attention is selective and attributes different weights to different words in the sequence. Using the context notation:\n",
    "   $$\n",
    "   c_i = \\sum\\limits_{j=1}^{T_x} \\alpha_{ij} h_j\n",
    "   $$\n",
    "3. Transformer architecture was later introduced in [\"Attention is all you need\" paper](https://arxiv.org/pdf/1706.03762), removing the need for RNNs altogether by utilising the concept of self-attention.\n",
    "   ![self attention](img/self_attention.png)\n",
    "4. Self-attention essentially adds additional context information to each input. This context information is used by the model in order to adjust the relative impact of each word on the resulting output.\n",
    "5. There are many types of self-attention mechanisms, the original one is named \"scaled dot-product attention\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a51f58-ab70-4ebc-a157-620c32f9193a",
   "metadata": {},
   "source": [
    "## Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17f0a1-17ea-4ad6-a605-43b0c2e2f812",
   "metadata": {},
   "source": [
    "### 1. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed4c02-fcd2-4680-98ee-571d13ed5646",
   "metadata": {},
   "source": [
    "Let's consider a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71fc4a5-4027-4f4d-a5e7-a8c6dea7349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Life is short, eat dessert first'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3ab09-0448-40fb-8e88-658218ac1607",
   "metadata": {},
   "source": [
    "Let's pretend that our vocabulary consists *only* of the words in the sentence above. We construct it via:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295a3f80-d1b2-4258-9771-4226b3b0c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91ebe0-3629-4047-87a5-877a70785a7d",
   "metadata": {},
   "source": [
    "Now we can use this dictionary to assign an index to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024f0766-1b51-4108-99f7-c09f55908abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1cf72-b06f-4d87-893b-eb6c9e13ab43",
   "metadata": {},
   "source": [
    "By now, we already have a vocabulary, `dc`, and a vector representation of the sentence, `sentence_int`. Let's use PyTorch's `Embedding` layer to construct an embedding for the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c80cf2-394b-4f43-96fd-a68af315158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
      "torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(6, 16)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc1518-e928-47b7-b4ef-5ccc35e9bfba",
   "metadata": {},
   "source": [
    "### 2. Weight matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3170c8d-fbbb-4d28-9cd6-2de13d15efe4",
   "metadata": {},
   "source": [
    "Self-attention relies on query, key, and value weight matrices.\n",
    "\n",
    "We'll denote them by $W_q$, $W_k$, and $W_v$, respectively.\n",
    "\n",
    "Two important things to keep mind:\n",
    "1. These are model parameters, therefore, are being **adjusted during training**.\n",
    "2. They are multiplied with inputs in order to obtain query, key, and value sequences via:\n",
    "   - Query sequence: $q^{(i)} = W_q x^{(i)}, \\; i = \\overline{0,T}$\n",
    "   - Key sequence: $k^{(i)} = W_k x^{(i)}, \\; i = \\overline{0,T}$\n",
    "   - Value sequence: $v^{(i)} = W_v x^{(i)}, \\; i = \\overline{0,T}$\n",
    "     \n",
    "   Here $T$ is the length of the input sequence.\n",
    "\n",
    "A visual representation:\n",
    "\n",
    "![attention matrices](img/attention-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba8747-9a22-4ea6-8e17-bc3c05a5a022",
   "metadata": {},
   "source": [
    "**Dimensions** of these are:\n",
    "- $x^{(i)}$ have length $d$\n",
    "- $W_q$ and $W_k$ are $d_k \\times d$\n",
    "- $W_v$ is $d_v \\times d$\n",
    "- $q^{(i)}$ and $q^{(i)}$ have lengths $d_k$\n",
    "- $q^{(i)}$ has length $d_v$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61ae34-1310-4f05-b93d-525a185f0667",
   "metadata": {},
   "source": [
    "**Note:** since we will be computing a product of query and key vectors $q^{(i)}$ and $k^{(i)}$, their dimensions are identical.\n",
    "\n",
    "Now the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71e5e57-2d2d-4201-9fcc-c7c9324f70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e5d13-ba5f-4f9d-8656-e42ee6f5f083",
   "metadata": {},
   "source": [
    "Let's compute the sequences for the **second input element**. It will act as a **query element** for subsequent computations, that is why it's shaded:\n",
    "\n",
    "![second_input](img/second_input_computation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f839977e-2cf6-4fd8-aed8-c27f6d7d1dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a40f91-6c35-4af7-8039-3721f81f7b1f",
   "metadata": {},
   "source": [
    "We can generalize the computation of key and value matrices for all sequence elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd52be47-9648-4be3-8990-7249cf65538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16a017-4e16-43b9-852f-52498e9fb571",
   "metadata": {},
   "source": [
    "### 3. Attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1da4d9-327a-4cf5-b32d-a9d6192ff6b5",
   "metadata": {},
   "source": [
    "Now we can compute **unnormalized attention weights** $\\omega$. These are defined as a product of query and key vectors:\n",
    "$$\n",
    "\\omega_{ij} = q^{(i)T} k^{(j)}\n",
    "$$\n",
    "\n",
    "![attention weights](img/attention-weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581850f-9c06-4b06-abde-a687069b2016",
   "metadata": {},
   "source": [
    "For instance, the computation of our query element and fifth key element is performed via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35066226-1566-4c7a-8ad7-2bdaf66eb597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1466, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_25 = query_2.dot(keys[4])\n",
    "print(omega_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8654002-85d3-4f8c-96d6-6895455ed00a",
   "metadata": {},
   "source": [
    "In matrix form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596f5b08-3b13-4592-ad27-60b08ffb3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d586e-63b9-49ce-a84b-027f536857c4",
   "metadata": {},
   "source": [
    "### 4. Attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd8fca-16d8-4c1c-93f0-5ea85945c691",
   "metadata": {},
   "source": [
    "Now we need to normalize the unnormalized attention weights $\\omega$. We will denote **normalized attention weights** by $\\alpha_{ij}$.\n",
    "\n",
    "Normalization is done in two steps:\n",
    "1. Divide $\\omega_{ij}$ by $\\sqrt{d_k}$, so that the length of weight vectors is within a certain amount.\n",
    "2. Apply softmax to the above.\n",
    "\n",
    "Visual representation:\n",
    "\n",
    "![attention scores](img/attention-scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec82481-fda1-413a-80d1-5ef637cc7dbd",
   "metadata": {},
   "source": [
    "The code for the computation described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76276d06-74bb-40a4-9949-48e0051b16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7b49f-0d24-45f9-a1b8-5fb23030d4c1",
   "metadata": {},
   "source": [
    "### 5. Context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb00903-0ef7-4942-a604-dddbceeecef8",
   "metadata": {},
   "source": [
    "And now the value vectors $v^{(i)}$ finally come into play. We will use them to compute a context vector $z^{(2)}$ for current input $x^{(2)}$ and current query $q^{(2)}$:\n",
    "$$\n",
    "z^{(2)} = \\sum\\limits_{j=1}^T \\alpha_{2,j} v^{(j)}\n",
    "$$\n",
    "\n",
    "![context_vector](img/context-vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b3bc1-4ccb-4197-b4c5-3f1dcbd992e5",
   "metadata": {},
   "source": [
    "The code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2a1c76-1b3f-4641-aa41-6514329b2c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098342bd-8091-4756-8fa4-9cc915efa408",
   "metadata": {},
   "source": [
    "To summarize, below are the computations and dimensions of different tensors involved in the process:\n",
    "\n",
    "![summary](img/summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2bcfb-0eee-40d7-8345-2b6e414a60ba",
   "metadata": {},
   "source": [
    "### 6. Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2eabf4-701d-44b8-a93f-35f159f9e0b7",
   "metadata": {},
   "source": [
    "So far we've implemented single-head attention model. \"Single-head\" means that we utilized a single set of (query, key, value) matrices:\n",
    "\n",
    "![single-head](img/single-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d0315-3590-4af3-af7d-c63b207d99a9",
   "metadata": {},
   "source": [
    "In multi-head attention, we use several sets of such matrices. This is similar to using multiple kernels (or filters) in convolutional neural networks:\n",
    "\n",
    "![multi-head](img/multi-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808fd733-66a1-4ec1-8354-e22ee606254e",
   "metadata": {},
   "source": [
    "Let's suppose we have $h=3$ attention heads. In code this will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e78146-befe-4b1d-87b5-0cfaef2b9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717112e1-0547-4c45-aee9-7f55e1887f01",
   "metadata": {},
   "source": [
    "Let's again consider $x^{(2)}$. Each query element will now be $3 \\times d_q$-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f565dc72-8a74-4cd9-bbcc-f57259d3b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf609f-421a-4488-86e0-2790049104c8",
   "metadata": {},
   "source": [
    "Key and value sequences are obtained via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ad446f-4ae1-4aca-8d98-7d775b6705c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258aba4f-1193-40df-a56e-17e1be8b642e",
   "metadata": {},
   "source": [
    "We need *all* key and value elements in order to compute attention scores for the second input element. So we first need to expand the input sequence embeddings to size 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4137d861-010b-4864-a5f2-ea56844c4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 6])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445fc04-2355-4a13-8dd9-0d7584be084e",
   "metadata": {},
   "source": [
    "Now we compute *all* keys and values via PyTorch's batch matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ca4945-99c4-4c37-b43f-b84edecc8311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285fb1a-bd6a-4bb0-b352-61d78a2542be",
   "metadata": {},
   "source": [
    "In order to make keys and values easier to interpret, we can swap their second and third dimensions, so that their shape is similar to the one of `embedded_sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac255fad-22ff-4edd-b266-ef0688dc28d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 6, 24])\n",
      "multihead_values.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a6e22-0f93-4469-8863-c9e7e8842fc1",
   "metadata": {},
   "source": [
    "Now we can follow the steps outlined for single-head attention in order to compute:\n",
    "\n",
    "1. Unscaled attention weights $\\omega$\n",
    "2. Scaled attention weights $\\alpha$\n",
    "3. $h$ $d_v$-dimensional context vectors $z_i^{(2)}, i = \\overline{1,h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fdb7d-deed-4efe-9864-5216b114d92f",
   "metadata": {},
   "source": [
    "Final question remains - how do we construct a single $d_v$-dimensional vector expected by the subsequent layers in the architecture? By utilizing an additional learned matrix $W^o$ with dimensions $hd_v \\times d_v$, which get multiplied with stacked $z_i^{(2)}$ matrices via:\n",
    "$$\n",
    "z^{(2)} = \\left(z_1^{(2)} \\frown \\dots \\frown z_h^{(2)}\\right)W^o\n",
    "$$\n",
    "Computation of weights/scores/final context vector for multi-head attention is left as an **exercise**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7172a69-9de0-40c8-9e10-93bcfc28ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4df6eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_queries.shape: torch.Size([3, 24, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_queries = torch.bmm(multihead_W_query, stacked_inputs)\n",
    "print(\"multihead_queries.shape:\", multihead_queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb6ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omega_multihead.shape: torch.Size([3, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "omega_multihead = multihead_keys.bmm(multihead_queries)\n",
    "print(\"omega_multihead.shape:\", omega_multihead.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4f99e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights.shape: torch.Size([3, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = F.softmax(omega_multihead, dim=-1)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4dce9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector_multihead.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "context_vector_multihead = attention_weights.bmm(multihead_values)\n",
    "print(\"context_vector_multihead.shape:\", context_vector_multihead.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c09285e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector_multihead.shape:  torch.Size([6, 28])\n",
      "\n",
      "context_vector_multihead: \n",
      "\n",
      " tensor([[-36.7080, -56.4900, -40.9637, -43.7478, -46.9687, -41.4747, -32.1160,\n",
      "         -40.7131, -39.0432, -40.3633, -37.5257, -30.6776, -39.4162, -44.5678,\n",
      "         -46.3646, -45.7392, -40.2715, -52.8047, -37.0162, -44.8804, -43.6007,\n",
      "         -33.2716, -37.1166, -47.2221, -36.0918, -51.2621, -42.0340, -44.0273],\n",
      "        [-42.4237, -35.1147, -40.3494, -36.0478, -52.3049, -37.6634, -47.6708,\n",
      "         -48.2758, -51.0518, -35.0632, -43.7947, -43.7721, -39.6691, -40.5930,\n",
      "         -40.5864, -43.6687, -56.3757, -44.8362, -42.8349, -45.0869, -32.9323,\n",
      "         -43.0648, -46.1633, -53.7393, -54.1361, -44.7839, -43.7404, -40.7336],\n",
      "        [-19.5519, -46.0659, -29.3029, -30.7946, -29.8873, -34.4560, -23.0608,\n",
      "         -36.9455, -33.1681, -23.6629, -22.5297, -21.1637, -29.9339, -27.6162,\n",
      "         -34.8649, -39.8651, -26.4951, -33.2307, -20.9430, -28.2029, -35.8101,\n",
      "         -17.8200, -21.2198, -34.8796, -30.6583, -48.5476, -29.0999, -30.8898],\n",
      "        [-31.0544, -22.5190, -32.3573, -30.5089, -40.3119, -27.0910, -30.0800,\n",
      "         -43.3553, -35.5027, -28.7455, -30.5359, -37.1797, -32.6180, -28.5369,\n",
      "         -32.4528, -29.0585, -41.0945, -35.5763, -33.6455, -24.7045, -29.5942,\n",
      "         -28.5156, -30.4440, -45.1697, -35.3818, -38.7353, -35.6638, -24.9360],\n",
      "        [-37.7002, -59.1155, -46.3724, -47.1447, -45.2066, -49.4999, -44.3074,\n",
      "         -47.1286, -44.1655, -42.4142, -36.4856, -35.3404, -41.1135, -46.6245,\n",
      "         -48.9512, -51.2478, -42.4143, -50.0186, -41.0047, -54.5333, -41.0750,\n",
      "         -36.6373, -33.0925, -51.0192, -41.3867, -52.5872, -42.4104, -44.4040],\n",
      "        [-43.7105, -36.2799, -44.2157, -39.9261, -51.9537, -42.2620, -48.2247,\n",
      "         -50.5519, -52.2010, -40.2982, -38.0743, -51.2048, -41.6143, -48.0111,\n",
      "         -40.8433, -42.5055, -56.0865, -52.7704, -47.8774, -47.5237, -37.1357,\n",
      "         -46.3946, -45.3020, -60.9225, -51.7401, -47.2125, -48.6646, -38.1708]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weight_o = torch.nn.Parameter(torch.rand(d_v * h, d_v))\n",
    "\n",
    "# Перетворення контекстних векторів\n",
    "context_vector_multihead = context_vector_multihead.reshape(-1, d_v * h).mm(weight_o)\n",
    "print(\"context_vector_multihead.shape: \", context_vector_multihead.shape)\n",
    "print(\"\\ncontext_vector_multihead: \\n\\n\", context_vector_multihead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8cf35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
