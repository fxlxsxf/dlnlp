{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8530d245-7473-4431-b91b-736ddd9593e6",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "1. **Get tokens** for positive and negative tweets (by `token` in this context we mean `word`).\n",
    "2. **Lemmatize** them (convert to base word forms). For that we will use a Part-of-Speech tagger.\n",
    "3. **Clean'em up** (remove mentions, URLs, stop words).\n",
    "4. **Prepare models** for the classifier, based on cleaned-up tokens.\n",
    "5. **Run the Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae761c-bd27-4d32-be19-f07965ced307",
   "metadata": {},
   "source": [
    "First, download necessary prepared samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42819310-88ea-463b-b6a8-cd88a312c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee03cc-4142-4362-85e6-508597ad4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88fbc7-e208-46cc-b748-eeb3a9191fe6",
   "metadata": {},
   "source": [
    "Get some sample positive/negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727ac170-6412-41e6-b6c6-1038906ffbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8c01a-fffb-42dd-bfe5-c0cf24bb7c38",
   "metadata": {},
   "source": [
    "We can either get the actual string content of those tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b8a199-3b39-4635-b658-375493490988",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae328604-e25e-4abf-a6b8-5d664f1438c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d184a10-a0be-477b-8e9a-36749a32730a",
   "metadata": {},
   "source": [
    "Or we can get a list of tokens using [tokenized method](https://www.nltk.org/howto/twitter.html) on `twitter_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd4b52e-d58e-4e5d-9275-0097185fbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff367ad-5a64-4ce9-b81b-e7c9bc9103a1",
   "metadata": {},
   "source": [
    "Now let's setup a Part-of-Speech tagger.  Download a perceptron tagger that will be used by the PoS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313b4107-4729-43c4-bb72-31be6498e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a170912-6686-4b4d-9e09-a6b56cb80e68",
   "metadata": {},
   "source": [
    "Import Part-of-Speech tagger that will be used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c64174c-0358-474c-a206-8f0038f7fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea756f9e-c15b-402c-9f80-bad6de2c25f3",
   "metadata": {},
   "source": [
    "Check how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a962113-b87c-40f9-896c-a2799447d4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f098854c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@groovinshawn',\n",
       " 'they',\n",
       " 'are',\n",
       " 'rechargeable',\n",
       " 'and',\n",
       " 'it',\n",
       " 'normally',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'a',\n",
       " 'charger',\n",
       " 'when',\n",
       " 'u',\n",
       " 'buy',\n",
       " 'it',\n",
       " ':)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f4c99-cb9a-40b1-a617-fe172051fa05",
   "metadata": {},
   "source": [
    "Let's write a function that will lemmatize twitter tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70d698-6050-4bd9-b365-66ef10e6de66",
   "metadata": {},
   "source": [
    "For that, let's first fetch a WordNet resource. WordNet is a semantically-oriented dictionary of English - check chapter 2.5 of the NLTK book. In online version, this is part 5 [here](https://www.nltk.org/book/ch02.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560e3e7b-a172-471c-8052-6dabbae85813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a3d4c-9080-4dbc-a077-2a6e234e03c8",
   "metadata": {},
   "source": [
    "Now fetch PoS tokens so that they can be passed to `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44c22c2-e6d3-47f4-a294-c3b78509d842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]\n",
    "\n",
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentence = []\n",
    "# Convert PoS tags into a format used by the lemmatizer\n",
    "# and run lemmatize\n",
    "for word, tag in pos_tag(tokens):\n",
    "    if tag.startswith('NN'):\n",
    "        pos = 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "        pos = 'v'\n",
    "    else:\n",
    "        pos = 'a'\n",
    "    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba42f25-399f-40bf-a7c0-d0ff912a2f3f",
   "metadata": {},
   "source": [
    "Note that it converts words to their base forms ('are' -> 'be', 'comes' -> 'come').\n",
    "\n",
    "Now we can proceed to processing. \n",
    "During processing, we will perform cleanup:\n",
    "  - remove URLs and mentions using regexes\n",
    "  - after lemmatization, remove *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "478174c9-8375-4722-88f7-8b551ed5ea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e47fe6-b896-4583-b971-50764e7484f9",
   "metadata": {},
   "source": [
    "What are these stopwords? Let's see some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4a45d9-327b-4d06-beb1-259809529c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc3201-6336-4a39-9628-bf76bf612b32",
   "metadata": {},
   "source": [
    "Here comes the `process_tokens` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2862fbeb-2a75-4b09-997d-1767c07e0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Now note the sheer size of regex for URLs :)\n",
    "        # Mentions regex is comparatively short and sweet\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token)):\n",
    "            continue\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "   \n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c7c49-46cf-4537-9fe4-750da9c38304",
   "metadata": {},
   "source": [
    "Let's test `process_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dafb0eb2-44c4-4d24-8bc8-2d97ed56e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880f61-72f3-4f61-b632-95df93e615dc",
   "metadata": {},
   "source": [
    "Run `process_tokens` on all positive/negative tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a4f99b-39e4-4764-83ab-8ad09dab51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92716ef9-e8d1-43d3-bc94-621362601f60",
   "metadata": {},
   "source": [
    "Let's see how did the processing go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d90aa3b-6606-4492-901c-6f26519e926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a7964-ae39-42a5-b8da-a4a9ba9912da",
   "metadata": {},
   "source": [
    "Let's see what is most common there. Add a helper function `get_all_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "400ab380-4422-48bb-a513-27ca12d3ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    return [w for tokens in cleaned_tokens_list for w in tokens]\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b74f0-34db-4295-b453-cb05dda8aefa",
   "metadata": {},
   "source": [
    "Perform frequency analysis using `FreqDist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05fa8805-6013-496e-8673-51308d607e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ad9c7-30dd-4f83-a307-7839fe03757e",
   "metadata": {},
   "source": [
    "Fine. Now we'll convert these to a data structure usable for NLTK's naive Bayes classifier ([docs here](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "790f1f82-d35d-45d9-90b3-52682501eb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e171ca9-67a1-424a-92c8-88cb4e47d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d61171-054b-468f-bc92-e57494690f9d",
   "metadata": {},
   "source": [
    "Create two datasets for positive and negative tweets. Use 7000/3000 split for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18964303-5c20-4a88-bcd1-df5f7e703aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb231b7-263f-45bb-abca-909bab161700",
   "metadata": {},
   "source": [
    "Finally we use the nltk's NaiveBayesClassifier on the training data we've just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a928308c-c221-4047-a3ff-dd56b2686c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9953333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2076.9 : 1.0\n",
      "                      :) = True           Positi : Negati =    984.0 : 1.0\n",
      "                follower = True           Positi : Negati =     22.0 : 1.0\n",
      "                     sad = True           Negati : Positi =     17.8 : 1.0\n",
      "                      aw = True           Negati : Positi =     14.5 : 1.0\n",
      "                     x15 = True           Negati : Positi =     13.1 : 1.0\n",
      "                  arrive = True           Positi : Negati =     12.9 : 1.0\n",
      "              appreciate = True           Positi : Negati =     12.2 : 1.0\n",
      "                     ugh = True           Negati : Positi =     11.8 : 1.0\n",
      "                    blog = True           Positi : Negati =     11.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8218b77-b2b0-4a0b-81bc-9d90cb11b154",
   "metadata": {},
   "source": [
    "Note the Positive:Negative ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb6996-3f21-4bbd-8a71-4e3c2f1eacdf",
   "metadata": {},
   "source": [
    "Let's check some test phrase. First, download punkt sentence tokenizer ([docs here](https://www.nltk.org/api/nltk.tokenize.punkt.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab87fd4-9909-469b-8c0a-3c4fd3a36199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ea904-26bb-4f64-9ec0-634ee5f1b7bd",
   "metadata": {},
   "source": [
    "Now we won't rely on `twitter_samples.tokenized`, but rather will use a generic tokenization routine - `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c24fbd-1dcd-4068-9831-c56c9482a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"the service was #bad\"\n",
    "\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(get_token_dict(custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bbbbb-7db6-4d20-8d8b-0d8c48ff79e9",
   "metadata": {},
   "source": [
    "Let's package it as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eea00802-a825-438b-b8ef-5c36519e6fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad :  Negative\n",
      "service is bad :  Positive\n",
      "service is really bad :  Negative\n",
      "service is so terrible :  Positive\n",
      "great service :  Positive\n",
      "they stole my money :  Negative\n",
      "#good :  Positive\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    custom_tokens = process_tokens(word_tokenize(text))\n",
    "    return classifier.classify(get_token_dict(custom_tokens))\n",
    "\n",
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\", \"#good\"]\n",
    "for t in texts:\n",
    "    print(t, \": \", get_sentiment(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f574c3-ed8a-442c-9f65-bcb50551c817",
   "metadata": {},
   "source": [
    "Seems ok!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff76e-bc19-4c2b-836e-bdddd013faad",
   "metadata": {},
   "source": [
    "### This is where the homework begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f60cd6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d25d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпортування модуля з відгуками на фільми\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "positive = movie_reviews.fileids('pos')\n",
    "negative = movie_reviews.fileids('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921f717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Розділення відгуків на позитивні та негативні\n",
    "positive_reviews = [nltk.corpus.movie_reviews.raw((positive[i])).replace(\"\\n\", \"\") for i in range(len(positive))]\n",
    "negative_reviews = [nltk.corpus.movie_reviews.raw((negative[i])).replace(\"\\n\", \"\") for i in range(len(negative))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ced973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підготовка масивів з токенами для подальшого їх використання в моделі\n",
    "positive_tokens_for_model = get_tweets_for_model([process_tokens(word_tokenize(positive_reviews[i])) for i in range(len(positive_reviews))])\n",
    "negative_tokens_for_model = get_tweets_for_model([process_tokens(word_tokenize(negative_reviews[i])) for i in range(len(negative_reviews))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b375673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створення позитивної та негативної вибірки для моделі\n",
    "positive_dataset = [(token, \"Positive\")\n",
    "                     for token in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(token, \"Negative\")\n",
    "                     for token in negative_tokens_for_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb52343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перемішування датасетів\n",
    "test_dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6876e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визначення точності на вибірці\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c969f",
   "metadata": {},
   "source": [
    "### Conclusion: on the dataset with the reviews, the model showed a poor result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eeec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens_test(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Now note the sheer size of regex for URLs :)\n",
    "        # Mentions regex is comparatively short and sweet\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token) or re.search(r'(#[A-Za-z0-9_]+)', token)):\n",
    "            continue\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "   \n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "positive_tweet_tokens_test = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens_test = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list_test = [process_tokens_test(tokens) for tokens in positive_tweet_tokens_test]\n",
    "negative_cleaned_tokens_list_test = [process_tokens_test(tokens) for tokens in negative_tweet_tokens_test]\n",
    "\n",
    "positive_tokens_for_model_test = get_tweets_for_model(positive_cleaned_tokens_list_test)\n",
    "negative_tokens_for_model_test = get_tweets_for_model(negative_cleaned_tokens_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2027a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset_test = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model_test]\n",
    "\n",
    "negative_dataset_test = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model_test]\n",
    "\n",
    "dataset_test = positive_dataset_test + negative_dataset_test\n",
    "\n",
    "random.shuffle(dataset_test)\n",
    "\n",
    "train_data_new = dataset_test[:7000]\n",
    "test_data_new = dataset_test[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_new = NaiveBayesClassifier.train(train_data_new)\n",
    "acc_hashon = []\n",
    "acc_hashoff = []\n",
    "for i in range(1500):\n",
    "    random.shuffle(dataset)\n",
    "    test_data = dataset[7000:]\n",
    "    acc_hashon.append(classify.accuracy(classifier, test_data))\n",
    "    acc_hashoff.append(classify.accuracy(classifier_new, test_data))\n",
    "\n",
    "print(\"Mean accuracy with hashtags is:\", sum(acc_hashon)/len(acc_hashon))\n",
    "print(\"Mean accuracy without hashtags is:\", sum(acc_hashoff)/len(acc_hashoff))\n",
    "if sum(acc_hashon)/len(acc_hashon) > sum(acc_hashoff)/len(acc_hashoff):\n",
    "    print(f\"Mean accuracy with hashtags is higher at: {round(100 * (sum(acc_hashon)/len(acc_hashon) - sum(acc_hashoff)/len(acc_hashoff)), 4)} %\")\n",
    "else:\n",
    "    print(f\"Mean accuracy without hashtags is higher at: {round(100 * (sum(acc_hashoff)/len(acc_hashoff) - sum(acc_hashon)/len(acc_hashon)), 4)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd7e4d",
   "metadata": {},
   "source": [
    "### Conclusion: a model trained on a dataset excluding hashtags yields more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a58be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [features for features, label in dataset]\n",
    "labels = [label for features, label in dataset]\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "log_reg_cls = LogisticRegression()\n",
    "\n",
    "pipeline = make_pipeline(vectorizer, log_reg_cls)\n",
    "\n",
    "accuracy_scores = cross_val_score(pipeline, features, labels, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Mean accuracy:\", accuracy_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214de124",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accuracy_scores.mean() < sum(acc_hashon)/len(acc_hashon):\n",
    "    print(f\"Mean accuracy NaiveBayesClassifier is higher then mean accuracy LogisticRegression\")\n",
    "else:\n",
    "    print(f\"Mean accuracy LogisticRegression is higher then mean accuracy NaiveBayesClassifier\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90a799",
   "metadata": {},
   "source": [
    "### Conclusion: NaiveBayesClassifier is better then LogisticRegression in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4eab64",
   "metadata": {},
   "source": [
    "### \"To speed up code execution, you can make the following changes:\n",
    "\n",
    "1. Reduce the number of loops.\n",
    "2. Whenever possible, use generators instead of loops.\"\n",
    "\n",
    "### \"To improve the model accuracy, you can do the following:\n",
    "\n",
    "1. Choose a different classification model.\n",
    "2. Utilize various hyperparameter tuning methods, such as GridSearchCV or RandomizedSearchCV.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462fa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebf48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
